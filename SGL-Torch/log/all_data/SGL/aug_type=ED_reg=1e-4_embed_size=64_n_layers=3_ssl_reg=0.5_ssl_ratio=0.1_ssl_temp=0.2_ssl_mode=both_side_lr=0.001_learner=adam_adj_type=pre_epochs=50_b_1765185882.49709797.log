2025-12-08 17:24:42.497: my pid: 4089406
2025-12-08 17:24:42.497: model: model.general_recommender.SGL
2025-12-08 17:24:42.497: Dataset statistics:
Name: all_data
The number of users: 2809
The number of items: 45331
The number of ratings: 85936
Average actions of users: 30.59
Average actions of items: 1.90
The sparsity of the dataset: 99.932512%

The number of training: 85936
The number of validation: 0
The number of testing: 0
2025-12-08 17:24:42.497: NeuRec:[NeuRec]:
recommender=SGL
dataset=all_data
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=50
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=100
pretrain_flag=0
save_flag=0

Command line:
recommender=SGL
dataset=all_data
aug_type=ED
reg=1e-4
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
2025-12-08 17:24:45.191: [iter 1 : loss = 10892.9740, bpr = 1416.6882, reg = 0.0052, time = 1.8446]
2025-12-08 17:24:46.814: [iter 2 : loss = 10791.2812, bpr = 1415.7774, reg = 0.0056, time = 1.6213]
2025-12-08 17:24:48.448: [iter 3 : loss = 10741.4442, bpr = 1414.9285, reg = 0.0060, time = 1.6331]
2025-12-08 17:24:50.020: [iter 4 : loss = 10713.9167, bpr = 1414.2353, reg = 0.0063, time = 1.5694]
2025-12-08 17:24:51.672: [iter 5 : loss = 10691.0573, bpr = 1413.6278, reg = 0.0065, time = 1.6507]
2025-12-08 17:24:53.223: [iter 6 : loss = 10679.9896, bpr = 1413.1489, reg = 0.0068, time = 1.5487]
2025-12-08 17:24:54.812: [iter 7 : loss = 10671.5394, bpr = 1412.6907, reg = 0.0070, time = 1.5880]
2025-12-08 17:24:56.393: [iter 8 : loss = 10661.8973, bpr = 1412.2193, reg = 0.0073, time = 1.5796]
2025-12-08 17:24:57.916: [iter 9 : loss = 10656.3341, bpr = 1411.7713, reg = 0.0076, time = 1.5214]
2025-12-08 17:24:59.500: [iter 10 : loss = 10654.7708, bpr = 1411.2895, reg = 0.0079, time = 1.5834]
2025-12-08 17:25:01.025: [iter 11 : loss = 10650.8423, bpr = 1410.8930, reg = 0.0082, time = 1.5234]
2025-12-08 17:25:02.629: [iter 12 : loss = 10643.6540, bpr = 1410.4191, reg = 0.0086, time = 1.6030]
2025-12-08 17:25:04.196: [iter 13 : loss = 10641.4777, bpr = 1409.9755, reg = 0.0089, time = 1.5658]
2025-12-08 17:25:05.851: [iter 14 : loss = 10638.3333, bpr = 1409.4604, reg = 0.0093, time = 1.6534]
2025-12-08 17:25:07.511: [iter 15 : loss = 10636.7865, bpr = 1408.9777, reg = 0.0097, time = 1.6585]
2025-12-08 17:25:09.078: [iter 16 : loss = 10635.8289, bpr = 1408.4487, reg = 0.0102, time = 1.5649]
2025-12-08 17:25:10.696: [iter 17 : loss = 10633.0082, bpr = 1407.8054, reg = 0.0106, time = 1.6161]
2025-12-08 17:25:12.249: [iter 18 : loss = 10629.6689, bpr = 1407.1079, reg = 0.0111, time = 1.5520]
2025-12-08 17:25:13.871: [iter 19 : loss = 10628.3251, bpr = 1406.4194, reg = 0.0117, time = 1.6212]
2025-12-08 17:25:15.491: [iter 20 : loss = 10625.0833, bpr = 1405.4980, reg = 0.0123, time = 1.6186]
2025-12-08 17:25:17.047: [iter 21 : loss = 10625.6719, bpr = 1404.7895, reg = 0.0128, time = 1.5536]
2025-12-08 17:25:18.668: [iter 22 : loss = 10626.0833, bpr = 1403.8673, reg = 0.0134, time = 1.6203]
2025-12-08 17:25:20.223: [iter 23 : loss = 10621.7016, bpr = 1402.7017, reg = 0.0142, time = 1.5538]
2025-12-08 17:25:21.814: [iter 24 : loss = 10620.9435, bpr = 1401.8409, reg = 0.0148, time = 1.5891]
2025-12-08 17:25:23.349: [iter 25 : loss = 10616.2411, bpr = 1400.2246, reg = 0.0158, time = 1.5346]
2025-12-08 17:25:24.938: [iter 26 : loss = 10616.3757, bpr = 1399.4076, reg = 0.0165, time = 1.5878]
2025-12-08 17:25:26.522: [iter 27 : loss = 10615.3192, bpr = 1397.8298, reg = 0.0174, time = 1.5826]
2025-12-08 17:25:28.055: [iter 28 : loss = 10609.6741, bpr = 1396.9152, reg = 0.0183, time = 1.5319]
2025-12-08 17:25:29.708: [iter 29 : loss = 10608.4658, bpr = 1394.8703, reg = 0.0194, time = 1.6517]
2025-12-08 17:25:31.259: [iter 30 : loss = 10607.6719, bpr = 1392.9488, reg = 0.0206, time = 1.5496]
2025-12-08 17:25:32.845: [iter 31 : loss = 10608.0647, bpr = 1391.5295, reg = 0.0217, time = 1.5842]
2025-12-08 17:25:34.425: [iter 32 : loss = 10603.0670, bpr = 1388.9398, reg = 0.0232, time = 1.5792]
2025-12-08 17:25:36.019: [iter 33 : loss = 10601.0357, bpr = 1387.5363, reg = 0.0242, time = 1.5925]
2025-12-08 17:25:37.658: [iter 34 : loss = 10597.3854, bpr = 1384.2949, reg = 0.0262, time = 1.6367]
2025-12-08 17:25:39.260: [iter 35 : loss = 10597.0528, bpr = 1382.1561, reg = 0.0277, time = 1.6002]
2025-12-08 17:25:40.861: [iter 36 : loss = 10594.2388, bpr = 1379.2549, reg = 0.0296, time = 1.5998]
2025-12-08 17:25:42.395: [iter 37 : loss = 10591.2522, bpr = 1376.5304, reg = 0.0314, time = 1.5319]
2025-12-08 17:25:43.994: [iter 38 : loss = 10585.6667, bpr = 1371.9804, reg = 0.0342, time = 1.5980]
2025-12-08 17:25:45.616: [iter 39 : loss = 10587.7545, bpr = 1369.0954, reg = 0.0362, time = 1.6209]
2025-12-08 17:25:47.189: [iter 40 : loss = 10579.5521, bpr = 1364.8880, reg = 0.0388, time = 1.5721]
2025-12-08 17:25:48.834: [iter 41 : loss = 10577.1726, bpr = 1359.6311, reg = 0.0423, time = 1.6438]
2025-12-08 17:25:50.461: [iter 42 : loss = 10573.6749, bpr = 1355.9824, reg = 0.0451, time = 1.6248]
2025-12-08 17:25:52.071: [iter 43 : loss = 10566.4070, bpr = 1351.2139, reg = 0.0483, time = 1.6087]
2025-12-08 17:25:53.641: [iter 44 : loss = 10563.5476, bpr = 1345.4649, reg = 0.0519, time = 1.5694]
2025-12-08 17:25:55.290: [iter 45 : loss = 10558.4539, bpr = 1339.6043, reg = 0.0562, time = 1.6478]
2025-12-08 17:25:56.907: [iter 46 : loss = 10552.2731, bpr = 1333.9211, reg = 0.0604, time = 1.6152]
2025-12-08 17:25:58.452: [iter 47 : loss = 10546.5335, bpr = 1325.9978, reg = 0.0656, time = 1.5432]
2025-12-08 17:26:00.101: [iter 48 : loss = 10538.3676, bpr = 1319.1484, reg = 0.0709, time = 1.6477]
2025-12-08 17:26:01.689: [iter 49 : loss = 10532.7403, bpr = 1312.6483, reg = 0.0753, time = 1.5863]
2025-12-08 17:26:03.301: [iter 50 : loss = 10525.5149, bpr = 1304.4220, reg = 0.0815, time = 1.6109]
2025-12-08 17:26:03.301: best_result@epoch 0:

2025-12-08 17:26:03.301: training finished without evaluation
2025-12-08 17:26:03.355: export_final_embeddings done: /mnt/sda1/sherry/BiGNAS/SGL-BiGNAS-new/SGL-Torch/dataset/all_data/pretrain-embeddings/SGL/final/
