2025-12-08 16:26:12.199: my pid: 4076433
2025-12-08 16:26:12.199: model: model.general_recommender.SGL
2025-12-08 16:26:12.199: Dataset statistics:
Name: all_data
The number of users: 2809
The number of items: 45331
The number of ratings: 85936
Average actions of users: 30.59
Average actions of items: 1.90
The sparsity of the dataset: 99.932512%

The number of training: 85936
The number of validation: 0
The number of testing: 0
2025-12-08 16:26:12.199: NeuRec:[NeuRec]:
recommender=SGL
dataset=all_data
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=50
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=100
pretrain_flag=0
save_flag=0

Command line:
recommender=SGL
dataset=all_data
aug_type=ED
reg=1e-4
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
2025-12-08 16:27:05.532: [iter 1 : loss = 457217.8125, bpr = 59500.8984, reg = 0.2201, time = 52.4927]
2025-12-08 16:27:57.329: [iter 2 : loss = 452946.5938, bpr = 59462.6445, reg = 0.2370, time = 51.6378]
2025-12-08 16:28:49.158: [iter 3 : loss = 450853.0625, bpr = 59426.9883, reg = 0.2520, time = 51.6591]
2025-12-08 16:29:41.016: [iter 4 : loss = 449697.4375, bpr = 59397.8672, reg = 0.2639, time = 51.6816]
2025-12-08 16:30:33.065: [iter 5 : loss = 448736.7812, bpr = 59372.3398, reg = 0.2742, time = 51.8804]
2025-12-08 16:31:25.028: [iter 6 : loss = 448271.3125, bpr = 59352.2109, reg = 0.2840, time = 51.8017]
2025-12-08 16:32:17.193: [iter 7 : loss = 447917.2812, bpr = 59332.9609, reg = 0.2942, time = 52.0037]
2025-12-08 16:33:09.348: [iter 8 : loss = 447512.0625, bpr = 59313.1602, reg = 0.3056, time = 51.9953]
2025-12-08 16:34:01.225: [iter 9 : loss = 447278.3750, bpr = 59294.3438, reg = 0.3181, time = 51.7186]
2025-12-08 16:34:53.067: [iter 10 : loss = 447212.5312, bpr = 59274.1172, reg = 0.3317, time = 51.6850]
2025-12-08 16:35:44.979: [iter 11 : loss = 447047.2500, bpr = 59257.4375, reg = 0.3448, time = 51.7457]
2025-12-08 16:36:36.943: [iter 12 : loss = 446745.5625, bpr = 59237.5469, reg = 0.3600, time = 51.8047]
2025-12-08 16:37:28.778: [iter 13 : loss = 446653.7500, bpr = 59218.9180, reg = 0.3749, time = 51.6775]
2025-12-08 16:38:20.654: [iter 14 : loss = 446521.6250, bpr = 59197.2695, reg = 0.3916, time = 51.7183]
2025-12-08 16:39:12.517: [iter 15 : loss = 446456.2188, bpr = 59176.9727, reg = 0.4082, time = 51.7066]
2025-12-08 16:40:04.415: [iter 16 : loss = 446416.1562, bpr = 59154.7656, reg = 0.4274, time = 51.7400]
2025-12-08 16:40:56.268: [iter 17 : loss = 446297.6562, bpr = 59127.7539, reg = 0.4467, time = 51.6928]
2025-12-08 16:41:48.181: [iter 18 : loss = 446157.6250, bpr = 59098.4609, reg = 0.4656, time = 51.7498]
2025-12-08 16:42:40.068: [iter 19 : loss = 446100.7812, bpr = 59069.5195, reg = 0.4895, time = 51.7306]
2025-12-08 16:43:32.016: [iter 20 : loss = 445964.5625, bpr = 59030.8398, reg = 0.5154, time = 51.7915]
2025-12-08 16:44:23.896: [iter 21 : loss = 445989.5312, bpr = 59001.0664, reg = 0.5372, time = 51.7217]
2025-12-08 16:45:15.879: [iter 22 : loss = 446006.7188, bpr = 58962.3320, reg = 0.5644, time = 51.8213]
2025-12-08 16:46:07.851: [iter 23 : loss = 445822.0312, bpr = 58913.3516, reg = 0.5966, time = 51.8149]
2025-12-08 16:46:59.855: [iter 24 : loss = 445790.7500, bpr = 58877.1914, reg = 0.6232, time = 51.8412]
2025-12-08 16:47:51.736: [iter 25 : loss = 445592.5000, bpr = 58809.2969, reg = 0.6647, time = 51.7202]
2025-12-08 16:48:43.655: [iter 26 : loss = 445598.5000, bpr = 58774.9688, reg = 0.6929, time = 51.7557]
2025-12-08 16:49:35.555: [iter 27 : loss = 445553.7188, bpr = 58708.7500, reg = 0.7329, time = 51.7397]
2025-12-08 16:50:27.602: [iter 28 : loss = 445317.1875, bpr = 58670.2617, reg = 0.7667, time = 51.8908]
2025-12-08 16:51:19.569: [iter 29 : loss = 445266.6562, bpr = 58584.4180, reg = 0.8143, time = 51.8070]
2025-12-08 16:52:11.556: [iter 30 : loss = 445232.8125, bpr = 58503.7422, reg = 0.8660, time = 51.8242]
2025-12-08 16:53:03.483: [iter 31 : loss = 445249.0938, bpr = 58444.1406, reg = 0.9111, time = 51.7696]
2025-12-08 16:53:55.078: [iter 32 : loss = 445039.3438, bpr = 58335.4102, reg = 0.9725, time = 51.4372]
2025-12-08 16:54:46.996: [iter 33 : loss = 444954.0312, bpr = 58276.3945, reg = 1.0173, time = 51.7593]
2025-12-08 16:55:38.955: [iter 34 : loss = 444801.4062, bpr = 58140.2617, reg = 1.1005, time = 51.7972]
2025-12-08 16:56:30.863: [iter 35 : loss = 444786.5938, bpr = 58050.4180, reg = 1.1627, time = 51.7494]
2025-12-08 16:57:22.744: [iter 36 : loss = 444668.4062, bpr = 57928.4844, reg = 1.2436, time = 51.7192]
2025-12-08 16:58:14.587: [iter 37 : loss = 444543.0625, bpr = 57814.2109, reg = 1.3176, time = 51.6869]
2025-12-08 16:59:06.471: [iter 38 : loss = 444308.9688, bpr = 57623.0156, reg = 1.4367, time = 51.7279]
2025-12-08 16:59:58.405: [iter 39 : loss = 444395.8438, bpr = 57501.7422, reg = 1.5206, time = 51.7771]
2025-12-08 17:00:50.319: [iter 40 : loss = 444051.1875, bpr = 57325.1289, reg = 1.6280, time = 51.7568]
2025-12-08 17:01:42.195: [iter 41 : loss = 443951.5938, bpr = 57104.2812, reg = 1.7769, time = 51.7195]
2025-12-08 17:02:34.096: [iter 42 : loss = 443804.7188, bpr = 56951.1055, reg = 1.8940, time = 51.7456]
2025-12-08 17:03:26.047: [iter 43 : loss = 443499.8750, bpr = 56751.0781, reg = 2.0273, time = 51.7879]
2025-12-08 17:04:18.042: [iter 44 : loss = 443379.6250, bpr = 56509.3906, reg = 2.1799, time = 51.8371]
2025-12-08 17:05:10.011: [iter 45 : loss = 443165.8438, bpr = 56263.4023, reg = 2.3599, time = 51.8119]
2025-12-08 17:06:02.016: [iter 46 : loss = 442906.1875, bpr = 56024.8516, reg = 2.5362, time = 51.8442]
2025-12-08 17:06:54.068: [iter 47 : loss = 442665.1875, bpr = 55692.1875, reg = 2.7559, time = 51.8903]
2025-12-08 17:07:46.139: [iter 48 : loss = 442322.5312, bpr = 55404.6133, reg = 2.9791, time = 51.9131]
2025-12-08 17:08:37.721: [iter 49 : loss = 442086.3125, bpr = 55131.8555, reg = 3.1631, time = 51.4212]
2025-12-08 17:09:29.737: [iter 50 : loss = 441781.7500, bpr = 54786.1719, reg = 3.4244, time = 51.8603]
2025-12-08 17:09:29.738: best_result@epoch 0:

2025-12-08 17:09:29.738: training finished without evaluation
2025-12-08 17:09:29.757: export_final_embeddings done: /mnt/sda1/sherry/BiGNAS/SGL-BiGNAS-new/SGL-Torch/dataset/all_data/pretrain-embeddings/SGL/final/
